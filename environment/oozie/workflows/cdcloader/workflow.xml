<?xml version='1.0' encoding='UTF-8'?>
<workflow-app xmlns='uri:oozie:workflow:0.5' name='${oozie.job.name}'>
	<global>
   		<job-tracker>${cluster.config.jobtracker}</job-tracker>
   		<name-node>${cluster.config.namespace}</name-node>
   		<configuration>
      		<property>
        		<name>mapreduce.job.queuename</name>
        		<value>${cluster.config.queuename}</value>
				</property>
   		</configuration>
	</global>
	<start to='cleanUp' />

	<!-- clean up from previous job -->
	<action name='cleanUp'>
		<fs>
			<delete path='${hdfs.paths.data.processing}'>
			<delete path='${hdfs.paths.data.output}'>
			<delete path='${hdfs.paths.control.temp}'>
			<mkdir path='${hdfs.paths.data.processing}'>
			<mkdir path='${hdfs.paths.data.output}'>
		</fs>
		<ok to='ProcessCDC' />
		<error to='fail' />
	</action>

	<!-- run the CDC Loader spark job -->
	<action name='ProcessCDC'>
		<spark xmlns='uri:oozie:spark-action:0.1'>
			<job-tracker>${cluster.config.jobtracker}</job-tracker>
            <name-node>${cluster.config.namespace}</name-node>
            <master>${spark.master}</master>
            <mode>${spark.mode}</mode>
            <name>${cdc.loader.job.name}</name>
            <class>${cdc.loader.class.main}</class>
            <jar>${spark.jar.location}</jar>
            <spark-opts>${spark.options}</spark-opts>
            <arg>${cdc.loader.options}</arg>
		</spark>
		<ok to='CDCControl'/>
        <error to='fail'/>
	</action>

	<!-- Run the CDC Control spark job (update the control table) -->
	<action name='CDCControl'>
		<spark xmlns='uri:oozie:spark-action:0.1'>
			<job-tracker>${cluster.config.jobtracker}</job-tracker>
            <name-node>${cluster.config.namespace}</name-node>
            <master>${spark.master}</master>
            <mode>${spark.mode}</mode>
            <name>${cdc.control.job.name}</name>
            <class>${cdc.control.main.class}</class>
            <jar>${spark.jar.location}</jar>
            <spark-opts>${spark.options}</spark-opts>
            <arg>${cdc.control.options}</arg>
		</spark>
		<ok to='MoveControlData'/>
        <error to='createErrorFiles'/>
	</action>
	<!-- move the control data to the control table folder -->
	<action name='MoveControlData'>
		<fs>
			<move source='${hdfs.paths.control.temp}' target='${hdfs.paths.control.output}'>
		</fs>
		<ok to='CopyArchiveData' />
		<error to='createErrorFiles' />
	</action>

	<!-- copy the data to the archive folder -->
	<action name='CopyArchiveData'>
		<distcp xmlns="uri:oozie:distcp-action:0.2">
			<job-tracker>${cluster.config.jobtracker}</job-tracker>
            <name-node>${cluster.config.namespace}</name-node>
            <arg>${hdfs.paths.data.processing}</arg>
            <arg>${hdfs.paths.data.archive}</arg>
		</distcp>
		<ok to='MoveCDCData' />
		<error to='createErrorFiles' />
	</action>

	<!-- move the data to the output folder -->
	<action name='MoveCDCData'>
		<fs>
			<move source='${hdfs.paths.data.processing}' target='${hdfs.paths.data.output}'>
		</fs>
		<ok to='end' />
		<error to='createErrorFiles' />
	</action>

	<!-- fail steps -->
	<action name='createErrorFiles'>
		<fs>
			<move source='${hdfs.paths.data.processing}' target='${hdfs.paths.data.error}'>
		</fs>
		<ok to='fail' />
		<error to='fail' />
	</action>

	<!-- kill steps -->
	<kill name="fail">
    <message>${oozie.job.name} failed, error
        message[${wf:errorMessage(wf:lastErrorNode())}]
    </message>
  </kill>

	<!-- end step -->
	<end name="end"/>
</workflow-app>

